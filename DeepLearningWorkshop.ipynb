{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningWorkshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rojiark/DeepLearningWorkshop/blob/master/DeepLearningWorkshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDnCOQtq_ZmC",
        "colab_type": "text"
      },
      "source": [
        "# Jupyter Notebook <a name=\"1\"></a>\n",
        "\n",
        "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages. There are two main cell types in a notebook:\n",
        "\n",
        "\n",
        "*   A **code cell** contains code to be executed in the kernel and displays its output below.\n",
        "*   A **markdown cell** contains text formatted using Markdown and displays its output in-place when it is run.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcHgkgc-MCFD",
        "colab_type": "text"
      },
      "source": [
        "## Code cells\n",
        "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_itSPSj_a3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "workshop_duration = 2 * 60 * 60\n",
        "print (\"Workshop duration: \", workshop_duration, \"seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrk4LUw_vZw",
        "colab_type": "text"
      },
      "source": [
        "To execute the code in the above cell, select it and execute the contents in the following ways:\n",
        "\n",
        "* Click the **Play icon** in the left gutter of the cell;\n",
        "* Type **Cmd/Ctrl+Enter** to run the cell in place;\n",
        "* Type **Shift+Enter** to run the cell and move focus to the next cell (adding one if none exists)\n",
        "* Type **Alt+Enter** to run the cell and insert a new code cell immediately below it.\n",
        "\n",
        "There are additional options for running some or all cells in the **Runtime** menu.\n",
        "\n",
        "All cells modify the same **global** state, so variables that you define by executing a cell can be used in other cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUCiUETu_w-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "workshop_duration = workshop_duration + (15 * 60)\n",
        "print (\"Workshop duration: \", workshop_duration, \"seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQTk_aX_C1ir",
        "colab_type": "text"
      },
      "source": [
        "You can also execute **bash commands** by adding an **\\!** sign at the start of the line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u07SGbiD730",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo Hello World"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWqUZ-CMI4wo",
        "colab_type": "text"
      },
      "source": [
        "**Magics** are shorthand annotations that change how a cell's text is executed. To learn more, see [Jupyter's magics page](http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZuE1ipcI4Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 30%; color: blue;'><b>Goooooooo Cobras!</b></marquee>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNjr4zb4F3G7",
        "colab_type": "text"
      },
      "source": [
        "## Markdown cells\n",
        "\n",
        "This is a **markdown cell**. You can **double-click** to edit this cell. Text cells\n",
        "use markdown syntax. To learn more, see the [markdown\n",
        "guide](/notebooks/markdown_guide.ipynb).\n",
        "\n",
        "You can also add math to text cells using [LaTeX](http://www.latex-project.org/)\n",
        "to be rendered by [MathJax](https://www.mathjax.org). Just place the statement\n",
        "within a pair of **\\$** signs.\n",
        "\n",
        "For example `$\\sqrt{3x-1}+(1+x)^2$` becomes\n",
        "$\\sqrt{3x-1}+(1+x)^2.$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaJHXHbBIBhO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Google Colaboratory <a name=\"2\"></a>\n",
        "\n",
        "Google Colab is an interactive document that lets you write, run and share Python code in Google Drive. You can think of colab as a **Jupyter Notebook** stored on the **cloud**. Colab connect the notebook to a cloud based runtime, meaning you can execute Python code withuot any setup or impact in your machine.\n",
        "\n",
        "Colab offers several code snippets for typical Python tasks available on the sidebar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTxi7JnLM0rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "interval = alt.selection_interval()\n",
        "\n",
        "alt.Chart(cars).mark_point().encode(\n",
        "  x='Horsepower',\n",
        "  y='Miles_per_Gallon',\n",
        "  color=alt.condition(interval, 'Origin', alt.value('lightgray'))\n",
        ").properties(\n",
        "  selection=interval\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_bM_qfGNEzk",
        "colab_type": "text"
      },
      "source": [
        "You can share notebooks via Google drive sharing or by exporting them to GitHub. The notebooks are saved in the standard jupyter notebook format, so they can be visualized with any compatible tool or framework.\n",
        "\n",
        "You can also upload and open files using the sidebar menu, or directly with a code cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shEYV_095_qL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1K5-HcX5_pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cut-t8JbN3RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "file = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whfNHSSjOcR-",
        "colab_type": "text"
      },
      "source": [
        "## Runtime\n",
        "\n",
        "You can connect to different runtimes with google colab with different locations and resources:\n",
        "\n",
        "\n",
        "*   Google Cloud runtime\n",
        "*   Local runtime\n",
        "*   Hosted runtime\n",
        "*   CPU / GPU / TPU runtime\n",
        "*   Python2 / Python3 runtime\n",
        "\n",
        "Let's find out the available resources in our current runtime:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niva8hTVPPu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USLLlOZpP0kV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXvI33kcRHcq",
        "colab_type": "text"
      },
      "source": [
        "To change the runtime type follow these steps:\n",
        "\n",
        "\n",
        "\n",
        "1.   Navigate to Runtime → Change runtime type\n",
        "2.   Select Hardware accelerator → GPU → SAVE\n",
        "3.   Navigate to Runtime → Reset all runtimes ... → YES\n",
        "4.   Now your runtime should have the GPU enabled\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Twyt4gSRZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8auiuKPBShAK",
        "colab_type": "text"
      },
      "source": [
        "Now repeat the same steps for TPU:\n",
        "\n",
        "1.   Navigate to Runtime → Change runtime type\n",
        "2.   Select Hardware accelerator → TPU → SAVE\n",
        "3.   Navigate to Runtime → Reset all runtimes ... → YES\n",
        "4.   Now your runtime should have the TPU enabled ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhDSndrfS3eE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1kmP03YS6iD",
        "colab_type": "text"
      },
      "source": [
        "**What is wrong?**\n",
        "\n",
        "TPUs provided by colab are not available in the same virtual machine, but rather provided as a service.\n",
        "\n",
        "The enviroment variable `COLAB_TPU_ADDR` provides the TPU cluster address for your runtime:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbf38uW1S6C4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "  print (os.environ['COLAB_TPU_ADDR'])\n",
        "except:\n",
        "  print ('COLAB_TPU_ADDR is not available in the runtime')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQe80MGkZcIq",
        "colab_type": "text"
      },
      "source": [
        "## Usage considerations\n",
        "\n",
        "*   Google colab is free to use\n",
        "*   There isn't currently a way to be charged / pay for more resources or computation time\n",
        "*   The runtime will reset after 10-15 minutes IDLE or 12 hours of continuous computation\n",
        "*   When the runtime resets you lose all the uploaded / saved files and environment \n",
        "*   Resourcess are asigned dynamically acording to the type of work you are doing\n",
        "*   The best available hardware is prioritized for users who use Colab interactively rather than for long-running computations. For example, the NVIDIA T4 GPU (\\$5000) can be replaced with an NVIDIA Tesla K80 (\\$900) if your computation is taking too long\n",
        "*   You can use applications such as ngrok to build a network tunnel and access your virtual machine from outside.\n",
        "*   Manage your active sessions with Runtime → Manage sessions\n",
        "*   To avoid loosing all your data when the runtime resets connect your google drive to Colab and save checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inq8n5hxbbmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this code to mount your google drive in your Colab virtual machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jRvJ5DbrSJg",
        "colab_type": "text"
      },
      "source": [
        "*   Stack overflow =)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjdaRP8PrQ-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqo8Ix9bfVsG",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow\n",
        "\n",
        "TensorFlow is an open-source machine learning library for research and production. TensorFlow offers APIs for beginners and experts to develop for desktop, mobile, web, and cloud. See the sections below to get started. Starting with Tensorflow 2.0, it includes the Keras API, a powerfull high level API that was initially a different project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEqjFW8ItePo",
        "colab_type": "text"
      },
      "source": [
        "## Example: MNIST digits dataset classification\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/datapythonista/mnist/master/img/samples.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
        "\n",
        "The MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\" In this example we will use a simple neural network to classify the dataset.\n",
        "\n",
        "Note that Google Colab already has Tensorflow 1.14.0 installed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fJOwnWDvd1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To determine which version you're using:\n",
        "!pip show tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3mzg2oAvg9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print (tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISmi_beMvw3y",
        "colab_type": "text"
      },
      "source": [
        "First, load and process the **dataset**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FjE5hlJvwJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjP-JmK3wJbM",
        "colab_type": "text"
      },
      "source": [
        "Create your **model** using the Keras API. The Keras API was integrated into tensorflow in version 1.14.0. Sequential models are the bread and butter of keras, a basic model with a single data flow, input, and output. This is a classic Neural network with one 512 neurons hidden layer and a dropout of 20%.\n",
        "\n",
        "\n",
        "*   **Flatten**: Transforms multidimensional into a 1D array. We only specify `input_shape` because it is the first layer of our model. Subsequent layers assume the input shape to be the same as the output of the previous layer\n",
        "*   **Dense**: Fully connected layer. The first parameter is the number of neurons and the second parameter their activation function.\n",
        "*   **Dropout**: A technique to avoid overfitting. Deactivate percentage of random connections on the network on each pass \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcaj_XHwx-SP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten(input_shape=x_train.shape[1:]))\n",
        "  model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqTuRG8C-dre",
        "colab_type": "text"
      },
      "source": [
        "**Train** the model using **CPU**. The **compile** function is used to configure the model for training, here we need to define the optimizer and loss function. Optionally we can add optimizer parameters and metrics. The **fit** function performs the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l13Qq4ji_c3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  model = create_model()\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  t = time.time()\n",
        "  model.fit(x_train, y_train, epochs=5)\n",
        "  cpu_time = time.time() - t\n",
        "  model.evaluate(x_test, y_test)\n",
        "\n",
        "print (\"CPU time: \", cpu_time, \"s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXq3x2V_Ar-r",
        "colab_type": "text"
      },
      "source": [
        "Now let's train the same model using a **Tensor Processing Units** (TPU). TPUs are harder to use than just a GPU because they are a cluster of nodes, you need to distribute work to each node. Fortunately, tensorflow has an API to distribute work to TPUs and other clusters.\n",
        "\n",
        "**tf.distribute.cluster_resolver**\n",
        "\n",
        "This library contains all implementations of ClusterResolvers. ClusterResolvers are a way of specifying cluster information for distributed execution. Built on top of existing ClusterSpec framework, ClusterResolvers are a way for TensorFlow to communicate with various cluster management systems (e.g. GCE, AWS, etc...).\n",
        "\n",
        "Classes:\n",
        "\n",
        "\n",
        "*   ClusterResolver: Abstract\n",
        "*   GCEClusterResolver: Google Compute Engine\n",
        "*   KubernetesClusterResolver: Google Containers\n",
        "*   SimpleClusterResolver: ClusterSpec\n",
        "*   SlurmClusterResolver: Slurm Workload Manager\n",
        "*   TFConfigClusterResolver: TF_CONFIG EnvVar\n",
        "*   **TPUClusterResolver**: TPU\n",
        "*   UnionResolver: Union of multiple resolvers\n",
        "\n",
        "TPUs support the following data types:\n",
        "\n",
        "*   tf.float32\n",
        "*   tf.complex64\n",
        "*   tf.int64\n",
        "*   tf.bool\n",
        "*   tf.bfloat64\n",
        "\n",
        "Let's use this API to initialize the TPU and create an strategy to train our model. **Change your runtime type to TPU, reset runtimes, and re-run the previous steps**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D3KzTi5Dlbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRCECPvDD6f2",
        "colab_type": "text"
      },
      "source": [
        "Now use the **strategy** scope to train your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMqIPXnpECey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  t = time.time()\n",
        "  model.fit(x_train.astype(np.float32), y_train.astype(np.float32), epochs=5) #TPUs don't support uint8\n",
        "  tpu_time = time.time() - t\n",
        "  model.evaluate(x_test.astype(np.float32), y_test.astype(np.float32))\n",
        "\n",
        "print (\"TPU time: \", tpu_time, \"s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruEtnrukHoFs",
        "colab_type": "text"
      },
      "source": [
        "**Why is TPU slower?**\n",
        "\n",
        "Currently each call to a TPU function copies the weights to the TPU before it can start running, this affects small operations more significantly. Smaller models will have worst performance on TPU, since it is optimized for large operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENHzvJ1YIoG2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now let's train on **GPU**, for this step forward we will be using Tensorflow 2.0 Release Candidate 0. On previous steps we used tf 1.14.0 because the cluster_resolver changed and it is not very stable, so TPU training was impossible.\n",
        "\n",
        "Change the runtime to GPU and Install **tf 2.0 rc0 gpu**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0I9VQO6IAsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-rc0\n",
        "\n",
        "# To determine which version you're using:\n",
        "!pip show tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQjCGkcJari",
        "colab_type": "text"
      },
      "source": [
        "Check that tf is seeing the GPU device and that the version imported is the right one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeHvs7x_Jk-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print (tf.test.gpu_device_name())\n",
        "print (tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP5edYtvPT7N",
        "colab_type": "text"
      },
      "source": [
        "Check the devices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0ZnMAzPPXeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm49YIZ0J3LV",
        "colab_type": "text"
      },
      "source": [
        "Now repeat the training with **GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZpm3Oc9sOhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten(input_shape=x_train.shape[1:]))\n",
        "  model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFPnnnS3J2JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  model = create_model()\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  t = time.time()\n",
        "  model.fit(x_train, y_train, epochs=5)\n",
        "  gpu_time = time.time() - t\n",
        "  model.evaluate(x_test, y_test)\n",
        "\n",
        "print (\"GPU time: \", gpu_time, \"s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nfzUWQ5LNoS",
        "colab_type": "text"
      },
      "source": [
        "Now save the model generated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQLfEEOeLRam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('./mnist.h5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tF74lznLpm9",
        "colab_type": "text"
      },
      "source": [
        "You can open a saved model from an hdf5 with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__2JMOXYL0NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_model = create_model()\n",
        "loaded_model.load_weights('./mnist.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjKWHdzlqSLv",
        "colab_type": "text"
      },
      "source": [
        "## Callbacks\n",
        "\n",
        "Callbacks are an important part of the keras API. They are utilities that can be called at certain points during model training.\n",
        "\n",
        "*   **BaseLogger**: Log metrics for each epoch\n",
        "*   **EarlyStopping**: Stop training when a certain metric is met\n",
        "*   **LearningRateScheduler**: Change the learning rate of the optimizer on the fly\n",
        "*   **ModelCheckpoint**: Save the model after every epoch\n",
        "*   **ReduceLROnPlateau**: Reduce learning rate when a certain metric is met\n",
        "*   **TensorBoard**: Enable visualizations for TensorBoard\n",
        "\n",
        "Early stopping and checkpoints are the most important callbacks to have in you model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZGEXtxiFXTC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Early stoping** prevents overfitting and overtraining to an extend. You define a metric, treshold, and condition, if it is met for a certain number of epochs, the model will automatically stop. IT is also usefull if you don't know how many epochs your training will take.\n",
        "\n",
        "Here is an example of early stopping with the same model we've been using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqGp26-BF4k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model()\n",
        "\n",
        "# Early stop callback\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.01, patience=2, mode='max', verbose=1)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=1000, callbacks = [early_stop])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsbDWlvuJdRF",
        "colab_type": "text"
      },
      "source": [
        "The **model checkpoint** callback creates a model checkpoint after each epoch or after a certain number of samples. It can also be configured to store only the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRdoeAwxJq3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model()\n",
        "\n",
        "# Model checkpoint callback\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"mnist_epoch_{epoch:02d}.h5\", monitor='accuracy', verbose=1, save_best_only=False, mode='max', save_freq='epoch')\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, callbacks = [checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA2FVn3ir1B-",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation\n",
        "Data augmentation is one of the most usefull trick when working with CNNs. Deep neural networks have a lot of parameters, so to prevent overfitting we need a lot of data. Something the available dataset is not big enough for the model we are trying to train, in these cases data augmentation can help. It consist on generating data with random scaling, rotation, croping and sometimes noise. A rule of thumb is that **the number of parameters and data should be around the same order of magnitude**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAz_uFnpQqv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "print (x_train.shape)\n",
        "print (y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFsQ6fw3PZA-",
        "colab_type": "text"
      },
      "source": [
        "**ImageDataGenerator** is a keras tool for data augmentation. This includes capabilities such as:\n",
        "\n",
        "*   Sample-wise standardization\n",
        "*   Feature-wise standardization\n",
        "*   ZCA whitening\n",
        "*   Random rotation, shifts, shear and flips\n",
        "*   Dimension reordering\n",
        "*   Save augmented images to disk\n",
        "\n",
        "Rather than performing the operations on the entire image dataset in memory, the API generates data during the training process. This reduces memory overhead, but adds computational cost during model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqrQgsUwU0ZE",
        "colab_type": "text"
      },
      "source": [
        "First plot the original data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZq8IdW0UlQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot\n",
        "for i in range(0, 9):\n",
        "\tpyplot.subplot(330 + 1 + i)\n",
        "\tpyplot.imshow(x_train[i].reshape(28,28), cmap=pyplot.get_cmap('gray'))\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-gd1KaGU5p5",
        "colab_type": "text"
      },
      "source": [
        "Now add std normalization, random rotations and shifts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ETNPDXQQWRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=True,\n",
        "                 featurewise_std_normalization=True,\n",
        "                 rotation_range=60.,\n",
        "                 width_shift_range=0.1,\n",
        "                 height_shift_range=0.1)\n",
        "\n",
        "# You can add a seed for reproducibility\n",
        "seed = 1\n",
        "datagen.fit(x_train, augment=True, seed=seed)\n",
        "\n",
        "# Plot augmented data\n",
        "for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\n",
        "\t# create a grid of 3x3 images\n",
        "\tfor i in range(0, 9):\n",
        "\t\tpyplot.subplot(330 + 1 + i)\n",
        "\t\tpyplot.imshow(x_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
        "\t# show the plot\n",
        "\tpyplot.show()\n",
        "\tbreak"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwCvmY63X89w",
        "colab_type": "text"
      },
      "source": [
        "We can use the data generator in training by calling **fit_generator** instead of fit. Or we can generate the dataset ourselves and pass it to fit:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIbGE8E1X8ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=10),\n",
        "                    steps_per_epoch=len(x_train) * 10, epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIhU1fhIoT0E",
        "colab_type": "text"
      },
      "source": [
        "# Tensorboard\n",
        "\n",
        "TensorBoard is an excellent tool for model and training visualization. The Keras API comes with several TensorBoard visualizations that can be used by installing **Tensoboard Callbacks**. We are going train a Keras model on Colab and visualize it while training with TensorBoard.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4SLdff0LgkL",
        "colab_type": "text"
      },
      "source": [
        "There is one thing we need to address before we start using tensorboard. Your Google Colab virtual machine is running on a local network located in a Google's server room, while your local machine could be anywhere else in the world. How to access the TensorBoard page from our local machine? We are going to use a free service named **ngrok** to tunnel the connection to your local machine.\n",
        "\n",
        "Here is a graph to show how it works:\n",
        "\n",
        "<img src=\"https://gitcdn.xyz/cdn/Tony607/blog_statics/d425c3fe4cf0d92067572e25ae6cc3198d51936b//images/ngrok/ngrok.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
        "\n",
        "Download and install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUOOdn5PLoqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPyaIEOVL3-h",
        "colab_type": "text"
      },
      "source": [
        "Run TensorBoard. Usually you would run the tensorboard server on localhost and connect from any browser, but here we are using ip 0.0.0.0. We use get_ipython().system_raw instead of the ! sign to be able to run in a separate thread (&)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W36DmajeL5JH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 127.0.0.1 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX0FwD5FMDGR",
        "colab_type": "text"
      },
      "source": [
        "Run ngrok. Also in a seaparate thread. The port must match the one passed to the tensorboard server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG09oNe4MFqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUfvPKj-ORv-",
        "colab_type": "text"
      },
      "source": [
        "Run the next command to obtain you network tunnel url. We will open this url in a browser after starting the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmp0MiKXOEsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwPeh9S3Ohnq",
        "colab_type": "text"
      },
      "source": [
        "Run a model with tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFVUafl9dav7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TensorBoard callback\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(\n",
        "                          log_dir='./log',\n",
        "                          histogram_freq=10,\n",
        "                          write_graph=True,\n",
        "                          write_images=True,\n",
        "                          update_freq=600000) #samples\n",
        "\n",
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', 'mse'])\n",
        "model.fit(x_train, y_train, epochs=1000, verbose=1, validation_data=(x_test, y_test), callbacks=[tensorboard])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xdbj3eGw6Jl",
        "colab_type": "text"
      },
      "source": [
        "Clean the runtime:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7SzzWwXwewa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ps aux | grep tensorboard\n",
        "!ps aux | grep ngrok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6XVDVQfwmjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill 881\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYYzo3f8w_M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXU2C7Qn4Kjk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*   ngrok has a limitation of **20 updates per minute**\n",
        "*   sparse_categorical_crossentropy metric is not working on training in TF 2.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29seuj6WoXpd",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning Basics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF4QQtxko0Zq",
        "colab_type": "text"
      },
      "source": [
        "## Machine learning problem types\n",
        "\n",
        "**Supervised**:\n",
        "**unsupervised**:\n",
        "**reinforced**:\n",
        "\n",
        "**Classification Tasks**: Image level classification, Region level classification (Detection, Localization), Pixel level classification (Segmentation)\n",
        "\n",
        "![Classification](https://developer.nvidia.com/sites/default/files/akamai/embedded/images/images/deep-vision-primitives.png)\n",
        "\n",
        "**Regression**: from a group of points, get the function that best describes the data\n",
        "\n",
        "![regression](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png)\n",
        "\n",
        "**Clustering**: from a group of points, get groups with similar characteristics\n",
        "\n",
        "![clusteringt](https://media.geeksforgeeks.org/wp-content/uploads/k-means-copy.jpg)\n",
        "\n",
        "**Anomaly detection**\n",
        "\n",
        "![anomaly](https://numenta.com/wp-content/uploads/2019/03/anomaly-detection-image.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8ynjKTgCVy2",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)\n",
        "\n",
        "**Image Analisys**\n",
        "\n",
        "Classical Pattern Recognition had the researchers manually analyse the dataset in order to determine the most important features.\n",
        "\n",
        "![alt text](https://neuralocean.de/wp-content/uploads/2018/03/patterrecognition-pipeline.png)\n",
        "\n",
        "Let us assume that we want to create a neural network model that is capable of recognizing swans in images. The swan has certain characteristics that can be used to help determine whether a swan is present or not, such as its long neck, its white color, etc.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/2824/1*tfw7GTZKq96uW2oDKy0PFw.png)\n",
        "\n",
        "For some images, it may be more difficult to determine whether a swan is present, consider the following image.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/2984/1*F4Hi-OtXMVs_y5-MxS97vA.png)\n",
        "\n",
        "Can it get any worse? It definitely can.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/2020/1*HCbbEQv9x5kIQc39SrWvAQ.png)\n",
        "![alt text](https://i.ibb.co/qsMXg3K/Image-from-i-OS.jpg)\n",
        "\n",
        "\n",
        "Humans were designing these feature detectors, and that made them either too simple or hard to generalize.\n",
        "\n",
        "*   What if we learned the features to detect?\n",
        "*   We need a system that can do Representation Learning (or Feature Learning).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aa0IOviEgAp",
        "colab_type": "text"
      },
      "source": [
        "**Traditional neural networks (Multilayer perceptron)**\n",
        "\n",
        "![alt text](https://miro.medium.com/max/500/1*BQ0SxdqC9Pl_3ZQtd3e45A.png)\n",
        "\n",
        "\n",
        "\n",
        "*   Activation\n",
        "*   Error\n",
        "*   Back propagation\n",
        "\n",
        "\n",
        "**How can we use classic NN with images?**\n",
        "\n",
        "**What problems do you see with classic NN?**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL4JX99zFQq5",
        "colab_type": "text"
      },
      "source": [
        "**CNN**\n",
        "\n",
        "CNN uses convolutions to resolve classic NN problems\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1950/1*p-_47puSuVNmRJnOXYPQCg.png)\n",
        "\n",
        "\n",
        "Classical NN:\n",
        "\n",
        "\n",
        "*   Do not scale well for images\n",
        "*   Ignore spatial information\n",
        "*   Cannot handle translations\n",
        "\n",
        "\n",
        "CNN’s:\n",
        "\n",
        "*   Pixel position has semantic meanings\n",
        "*   Elements of interest can appear anywhere in the image\n",
        "\n",
        "**Feature learning**\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1910/1*fLGuAUT5imTIGAeA4zzaWA.png)\n",
        "\n",
        "**Standard CNN**\n",
        "![alt text](https://miro.medium.com/max/2640/0*jXPgL1T2Gu5L4WSp.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3noP3PoOBtI",
        "colab_type": "text"
      },
      "source": [
        "**Example:**\n",
        "\n",
        "In this example, you can try out using tf.keras and Cloud TPUs to train a model on the fashion MNIST dataset. **This examples uses a TPU runtime**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EozibecWOvpx",
        "colab_type": "text"
      },
      "source": [
        "### Download data\n",
        "\n",
        "Begin by downloading the fashion MNIST dataset using `tf.keras.datasets`, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmeANtTfOu0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import distutils\n",
        "if distutils.version.LooseVersion(tf.__version__) < '1.14':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/fashion_mnist.ipynb')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# add empty color dimension\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7MoRMqoPM5Z",
        "colab_type": "text"
      },
      "source": [
        "### Build the model\n",
        "The following model has some errors, find those erros and add the necessary layers to improve the inference results:\n",
        "\n",
        "Hints:\n",
        "\n",
        "\n",
        "*   Can the model learn complex features with only one convolutional layer?\n",
        "*   How to prevent overfitting?\n",
        "*   Are the activation layers ok?\n",
        "*   `relu` removes negative values. what happens if we use relu on the input layer?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-g6kiV4PqjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(10))\n",
        "  model.add(tf.keras.layers.Activation('relu'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGUlM5s9P4br",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ofs9nnaP-K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    x_train.astype(np.float32), y_train.astype(np.float32),\n",
        "    epochs=17,\n",
        "    steps_per_epoch=60,\n",
        "    validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\n",
        "    validation_freq=17\n",
        ")\n",
        "\n",
        "model.save_weights('./fashion_mnist.h5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sGDWyM6P_v3",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdSu3HetQG9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
        "\n",
        "\n",
        "cpu_model = create_model()\n",
        "cpu_model.load_weights('./fashion_mnist.h5')\n",
        "\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_predictions(images, predictions):\n",
        "  n = images.shape[0]\n",
        "  nc = int(np.ceil(n / 4))\n",
        "  f, axes = pyplot.subplots(nc, 4)\n",
        "  for i in range(nc * 4):\n",
        "    y = i // 4\n",
        "    x = i % 4\n",
        "    axes[x, y].axis('off')\n",
        "    \n",
        "    label = LABEL_NAMES[np.argmax(predictions[i])]\n",
        "    confidence = np.max(predictions[i])\n",
        "    if i > n:\n",
        "      continue\n",
        "    axes[x, y].imshow(images[i])\n",
        "    axes[x, y].text(0.5, 0.5, label + '\\n%.3f' % confidence, fontsize=14)\n",
        "\n",
        "  pyplot.gcf().set_size_inches(8, 8)  \n",
        "\n",
        "plot_predictions(np.squeeze(x_test[:16]), \n",
        "                 cpu_model.predict(x_test[:16]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voadIFSao9ei",
        "colab_type": "text"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/3/3b/The_LSTM_cell.png)\n",
        "\n",
        "In this example, you train the model on the combined works of William Shakespeare, then use the model to compose a play in the style of *The Great Bard*:\n",
        "\n",
        "<blockquote>\n",
        "Loves that led me no dumbs lack her Berjoy's face with her to-day.  \n",
        "The spirits roar'd; which shames which within his powers  \n",
        "\tWhich tied up remedies lending with occasion,  \n",
        "A loud and Lancaster, stabb'd in me  \n",
        "\tUpon my sword for ever: 'Agripo'er, his days let me free.  \n",
        "\tStop it of that word, be so: at Lear,  \n",
        "\tWhen I did profess the hour-stranger for my life,  \n",
        "\tWhen I did sink to be cried how for aught;  \n",
        "\tSome beds which seeks chaste senses prove burning;  \n",
        "But he perforces seen in her eyes so fast;  \n",
        "And _  \n",
        "</blockquote>\n",
        "\n",
        "**This example uses a TPU runtime**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRvJMBFSLhLa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Download data\n",
        "\n",
        "Download *The Complete Works of William Shakespeare* as a single text file from [Project Gutenberg](https://www.gutenberg.org/). You use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULO423ZgJcwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJaAmdz1MOtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import distutils\n",
        "if distutils.version.LooseVersion(tf.__version__) < '1.14':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/shakespeare_with_tpu_and_keras.ipynb')\n",
        "\n",
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
        "\n",
        "def transform(txt):\n",
        "  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
        "\n",
        "def input_fn(seq_len=100, batch_size=1024):\n",
        "  \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n",
        "  with tf.io.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
        "    txt = f.read()\n",
        "\n",
        "  source = tf.constant(transform(txt), dtype=tf.int32)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1, drop_remainder=True)\n",
        "\n",
        "  def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "  BUFFER_SIZE = 10000\n",
        "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  return ds.repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZ3MdbSJ5cv",
        "colab_type": "text"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "The model is defined as a two-layer, forward-LSTM, the same model should work both on CPU and TPU.\n",
        "\n",
        "Because our vocabulary size is 256, the input dimension to the Embedding layer is 256.\n",
        "\n",
        "When specifying the arguments to the LSTM, it is important to note how the stateful argument is used. When training we will make sure that `stateful=False` because we do want to reset the state of our model between batches, but when sampling (computing predictions) from a trained model, we want `stateful=True` so that the model can retain information across the current batch and generate more interesting text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHfpnApKJTiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
        "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
        "  source = tf.keras.Input(\n",
        "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
        "  return tf.keras.Model(inputs=[source], outputs=[predicted_char])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZacCclALYco",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm9Pqu9WLnNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  training_model = lstm_model(seq_len=100, stateful=False)\n",
        "  training_model.compile(\n",
        "      optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "training_model.fit(\n",
        "    input_fn(),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10\n",
        ")\n",
        "training_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv9IxmSaOccO",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ1BqT5zO1wg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "\n",
        "# We seed the model with our initial string, copied BATCH_SIZE times\n",
        "\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "  prediction_model.predict(seed[:, i:i + 1])\n",
        "\n",
        "# Now we can accumulate predictions!\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "  \n",
        "  # sample from our output distribution\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  \n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  generated = ''.join([chr(c) for c in p])  # Convert back to text\n",
        "  print(generated)\n",
        "  print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N175uKDxN0Bj",
        "colab_type": "text"
      },
      "source": [
        "# More example notebooks\n",
        "\n",
        "- [Shakespeare in 5 minutes with Cloud TPUs and Keras](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb)\n",
        "- [Shakespeare in 5 minutes with Cloud TPUs via TPUEstimator](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpuestimator.ipynb)\n",
        "- [Fashion MNIST with Keras and TPUs](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb)\n",
        "- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n",
        "- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n",
        "- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n",
        "- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n",
        "- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."
      ]
    }
  ]
}